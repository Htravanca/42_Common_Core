ok valgrind --leak-check=full --show-leak-kinds=all ./a.out file1.txt \"cat\" \"grep foo\" \"wc -l\" file2.txt
ok mudar o return values do Exit para os codes corretos

ok ./pipex_bonus file1 "head -n 5" "grep test" "cut -d ' ' -f 1" file2
ok o split ja conta bem as palavras agora falta por o escrever bem

ok 
valgrind --trace-children=yes --leak-check=full --show-leak-kinds=all --track-fds=yes ./pipex file1.txt "pwd" "wc -l" "    " file2.txt
valgrind --trace-children=yes --leak-check=full --show-leak-kinds=all --track-fds=yes ./pipex infile "pwd" "wc -l" file2.txt

ok
echo $? - check the exit codes.

OK PATH=' ' ./pipex infile "ls" "wc -l" outfile

OK unset PATH
/bin/valgrind


some excellent code, handling aspas well (minishell level skills). 
Just a couple of edge cases that resulted in FDs left open and a parallelism issue 
too many open FDs (only in mandatory - bonus is working) when: infile doesn't exist infile/outfile has insufficient permissions
parallelism doesn't seem to be working in bonus (but it does in mandatory): sleep 5 sleep 5 sleep 5 -> sleeps for 15, should sleep for 5

Erro tester ./
./pipex "$input" "cat" "grep name" "grep -E a$" "wc -l" "$output_b"

Erro caso o Grep nao encontre bem o que procura
➜  pipexV2 git:(main) ✗ < infile cat | grep foo | wc -l > file2.txt
➜  pipexV2 git:(main) ✗ echo $?
0
➜  pipexV2 git:(main) ✗ ./pipex infile "cat" "grep foo" "wc -l" file2.txt
➜  pipexV2 git:(main) ✗ echo $?
1

error opening files return 0?

OK
error opening infile -> exit 0
error openignoutfile -> exit 1

error in first comand -> exit 0

Very good explanation unfortunately you are not handling absolute paths like /usr/bin/cat 
And careful when you do env -i and try with an absolute path aswell 
Therefore very good work and explanation and keep up the good work :)

PATH=' '
unset PATH
env -I

Quando dou PATH='' se der o primeiro comando com absolute path ele deteta e faz tudo-.....



-------------Parte mandatoria-----------------------------------
TESTE 1
./pipex /dev/stdin cat ls /dev/stdout
ls

valgrind --track-fds=all --trace-children=yes

"grep foo"
wc -l
-------------Parte bonus-----------------------------------
TESTE 1
./pipex file1.txt "cat" "grep foo" "wc -l" file2.txt
< file1.txt cat | grep foo | wc -l > file2.txt

TESTE 2
./pipex file1.txt "cat" "invalid_command" "wc -l" file2.txt

TESTE 3
./pipex file1.txt "cat" "grep something" "exit 1" file2.txt

TESTE 4
./pipex file1.txt "head -n 5" "grep test" "cut -d ' ' -f 1" file2.txt

TESTE 5
./pipex emptyfile "cat" "wc -l" "echo Done" file2.txt

TESTE 6 
./pipex file1.txt "tr a-z A-Z" "sort" "uniq" file2.txt

TESTE 7
./pipex file1.txt "cat" "grep hello" "sed 's/hello/world/'" "sort" "wc -l" file2.txt

TESTE 8
./pipex file1.txt "echo 'This is a test'" "tr -d 'aeiou'" "rev" file2.txt


  Test 4. Du, Sort, Head, and WC
        ORIGINAL - < infile du -sh * | sort -hr | head -5 | wc -l > outfile
        PIPEX - ./pipex infile "du -sh *" "sort -hr" "head -5" "wc -l" outfile


Test 6. Out of scope arguments
        ORIGINAL - < Makefile grep $ | 'awk '{ if (length($0) > max) max = length($0) } END { print max }'
        PIPEX - ./pipex Makefile "grep $" "'awk '{ if (length($0) > max) max = length($0) } END { print max }'" outfile.txt

        Explanation: This test provides complex and nested commands to ensure the program can handle more advanced and potentially out-of-scope commands.





I see a couple of areas in your code that need adjustments for handling pipes correctly, and I’ll also address the issue causing the infinite loop.
1. Pipe Closure Management

You are not properly closing pipes in both the parent and child processes. In a pipe, each process must close the file descriptors it doesn't need to avoid potential deadlocks. Specifically:

    The child process should close the pipe read-end (fd[0]).
    The parent process should close the pipe write-end (fd[1]).

2. Handling /dev/urandom Issue

Your infinite loop is likely due to the fact that /dev/urandom is an infinite stream of random data. This means the cat command will never finish reading. A solution would be to limit the read size or detect when to stop reading.

You can achieve this by handling the signal (like SIGPIPE) or limiting the output size by using head -1 to cut the output. However, the cat command doesn't know when to stop because you're reading from /dev/urandom, which never ends.
3. Redundant Open of Output File in main

You are opening the output file twice: once in main and once in ft_loop_process. The first open is unnecessary since ft_loop_process handles it correctly.
4. Dynamic Array for pid

Currently, you declare a large fixed-size array for PIDs (pid[10000]). Instead, it's better to dynamically allocate the array based on the number of commands.
5. Fixing Pipe Management and Execution

Let me restructure your code with proper pipe management, dynamic PID handling, and a strategy to avoid the infinite loop.
Corrected Code:

c

void	ft_execute(char *argv, char **envp)
{
	char	**cmdsarr;
	char	*path;

	cmdsarr = ft_split_adapt(argv, ' ');
	ft_split_result(cmdsarr);
	path = ft_path(cmdsarr, envp);
	if (!path)
	{
		perror("Command not found");
		ft_free_arr(cmdsarr);
		exit(127);
	}
	execve(path, cmdsarr, envp);
	perror("Error executing the cmd");
	ft_free_arr(cmdsarr);
	free(path);
	exit(1);
}

int	ft_execute_child(char *argv, char **envp, int input_fd, int output_fd)
{
	int	pid;

	pid = ft_handle_error(fork(), "Fork error");
	if (pid == 0)
	{
		// Child process
		dup2(input_fd, STDIN_FILENO);  // Redirect input
		dup2(output_fd, STDOUT_FILENO);  // Redirect output
		ft_execute(argv, envp);
	}
	return (pid);
}

static void	ft_wait(int *pid, int i)
{
	int	j;
	int	status;

	j = 0;
	while (j <= i)
	{
		if (waitpid(pid[j], &status, 0) < 0)
		{
			perror("Error waiting for child process");
			exit(1);
		}
		if (WEXITSTATUS(status) != 0)
			exit(WEXITSTATUS(status));
		j++;
	}
}

static void	ft_loop_process(int argc, char **argv, char **envp)
{
	int	*pid;
	int	i;
	int	fd[2];
	int	input_fd;

	// Allocate memory for PIDs based on the number of commands
	pid = malloc(sizeof(int) * (argc - 3));

	// Open input file
	input_fd = ft_handle_error(open(argv[1], O_RDONLY), "Error opening file1");

	// Loop through the commands and execute each in a pipeline
	i = 2;
	while (i < (argc - 2))
	{
		pipe(fd); // Create a new pipe

		pid[i - 2] = ft_execute_child(argv[i], envp, input_fd, fd[1]);

		close(fd[1]);  // Close the write-end of the pipe in the parent
		input_fd = fd[0];  // Update input_fd to be the read-end for the next command

		i++;
	}

	// Handle last command (redirect output to file)
	int wfd = ft_handle_error(open(argv[argc - 1], O_WRONLY | O_CREAT | O_TRUNC, 0644), "Error opening file2");
	pid[i - 2] = ft_execute_child(argv[argc - 2], envp, input_fd, wfd);

	close(wfd); // Close output file
	close(input_fd); // Close the last pipe input

	// Wait for all child processes to finish
	ft_wait(pid, (i - 2));

	// Free dynamically allocated memory
	free(pid);
}

int	main(int argc, char **argv, char **envp)
{
	if (argc >= 5)
		ft_loop_process(argc, argv, envp);
	else
		perror("Error ARGS, correct usage: ./pipex file1 cmd1 cmdn... file2");
	return (0);
}

Key Corrections:

    Proper Pipe Management:
        After each fork, the parent process closes the write-end of the pipe and the child process uses dup2 to replace standard input/output.
        The parent closes the read-end of the pipe after the next child gets it.

    Dynamic PID Allocation:
        PIDs are stored dynamically based on the number of commands using malloc instead of a large fixed array.

    Avoid Infinite Loop:
        The infinite loop happens because cat is reading from /dev/urandom, which doesn't terminate. You could use commands like head -c 100 to limit the number of characters read, or handle the situation with signals like SIGPIPE. For example, replace cat with head -n 1 to only take one line from the stream.

Test with:

bash

./pipex /dev/urandom "head -n 1" "head -n 1" outfile

This will prevent the process from hanging in an infinite loop
	